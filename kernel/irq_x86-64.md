# 中断 - x86-64

* arch/x86/include/asm/page_64_types.h
```cpp
#ifdef CONFIG_KASAN
#define KASAN_STACK_ORDER 1
#else
#define KASAN_STACK_ORDER 0
#endif

#define THREAD_SIZE_ORDER   (2 + KASAN_STACK_ORDER)
#define THREAD_SIZE  (PAGE_SIZE << THREAD_SIZE_ORDER)

#define EXCEPTION_STACK_ORDER (1 + KASAN_STACK_ORDER)
#define EXCEPTION_STKSZ (PAGE_SIZE << EXCEPTION_STACK_ORDER)

#define IRQ_STACK_ORDER (2 + KASAN_STACK_ORDER)
#define IRQ_STACK_SIZE (PAGE_SIZE << IRQ_STACK_ORDER)
```
* 在未开启 KASAN 的情况下：
  * 内核进程栈的大小`THREAD_SIZE`：`16kB`（`2^THREAD_SIZE_ORDER = 4`）
  * 中断栈的大小`IRQ_STACK_SIZE`：`16kB`（`2^IRQ_STACK_ORDER = 4`）
  * 异常栈的大小`EXCEPTION_STKSZ`：`8kB`（`2^EXCEPTION_STACK_ORDER = 2`）
* 有 KASAN 时会多一页

* arch/x86/include/asm/processor.h
```cpp
/*
 * Note that while the legacy 'TSS' name comes from 'Task State Segment',
 * on modern x86 CPUs the TSS also holds information important to 64-bit mode,
 * unrelated to the task-switch mechanism:
 */
#ifdef CONFIG_X86_32
...
#else
struct x86_hw_tss {
    u32         reserved1;
    u64         sp0;
    u64         sp1;

    /*
     * Since Linux does not use ring 2, the 'sp2' slot is unused by
     * hardware.  entry_SYSCALL_64 uses it as scratch space to stash
     * the user RSP value.
     */
    u64         sp2;

    u64         reserved2;
    u64         ist[7];
    u32         reserved3;
    u32         reserved4;
    u16         reserved5;
    u16         io_bitmap_base;

} __attribute__((packed));
#endif
...
struct tss_struct {
    /*
     * The fixed hardware portion.  This must not cross a page boundary
     * at risk of violating the SDM's advice and potentially triggering
     * errata.
     */
    struct x86_hw_tss   x86_tss;

    struct x86_io_bitmap    io_bitmap;
} __aligned(PAGE_SIZE);

DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);

/* Per CPU interrupt stacks */
struct irq_stack {
    char        stack[IRQ_STACK_SIZE];
} __aligned(IRQ_STACK_SIZE);
```

## 中断栈
* 对于无需进行 privilege-level 变化的情况，比如中断发生时 CPU 运行在内核态
  1. CPU 依然会将被中断进程的`ss`、`rsp`、`rflags`、`cs`、`rip`、`error code`等压入当前进程的栈
  2. 软件保存被中断进程的通用目的寄存器在当前进程栈，然后清除 GPRs 的内容
  3. 软件将栈切换到预设好的 PerCPU 的中断栈，栈的地址由 PerCPU 变量来记录，类似`moveq PER_CPU_VAR(irq_stack_ptr), %rsp`

## 异常栈
* 对于无需进行 privilege-level 变化的情况，比如异常发生时 CPU 运行在内核态
  1. CPU 依然会将被中断进程的`ss`、`rsp`、`rflags`、`cs`、`rip`、`error code`等压入当前进程的栈
  2. 软件保存被中断进程的通用目的寄存器在当前进程栈，然后清除 GPRs 的内容
  3. 软件在当前进程栈进行异常处理

## IST 异常栈
* 每种 IST 异常栈的是独立分配的，新版本加入了 guard page
* 由硬件完成 IST 异常栈的切换
* arch/x86/include/asm/page_64_types.h
```cpp
/*
 * The index for the tss.ist[] array. The hardware limit is 7 entries.
 */
#define IST_INDEX_DF        0
#define IST_INDEX_NMI       1
#define IST_INDEX_DB        2
#define IST_INDEX_MCE       3
#define IST_INDEX_VC        4
```
* 通过`ISTG()`定义了 IST 异常栈的起始地址
* arch/x86/include/asm/desc_defs.h
```cpp
struct idt_bits {
    u16     ist : 3,
            zero    : 5,
            type    : 5,
            dpl : 2,
            p   : 1;
} __attribute__((packed));

struct idt_data {
    unsigned int    vector;
    unsigned int    segment;
    struct idt_bits bits;
    const void  *addr;
};
```
* `ist`域为 IST 栈的索引
* arch/x86/kernel/idt.c
```cpp
#define G(_vector, _addr, _ist, _type, _dpl, _segment)  \
    {                       \
        .vector     = _vector,      \
        .bits.ist   = _ist,         \
        .bits.type  = _type,        \
        .bits.dpl   = _dpl,         \
        .bits.p     = 1,            \
        .addr       = _addr,        \
        .segment    = _segment,     \
    }
...
#ifdef CONFIG_X86_64
/*
 * Interrupt gate with interrupt stack. The _ist index is the index in
 * the tss.ist[] array, but for the descriptor it needs to start at 1.
 */
#define ISTG(_vector, _addr, _ist)          \
    G(_vector, _addr, _ist + 1, GATE_INTERRUPT, DPL0, __KERNEL_CS)
#else
#define ISTG(_vector, _addr, _ist)  INTG(_vector, _addr)
#endif
/*
 * The default IDT entries which are set up in trap_init() before
 * cpu_init() is invoked. Interrupt stacks cannot be used at that point and
 * the traps which use them are reinitialized with IST after cpu_init() has
 * set up TSS.
 */
static const __initconst struct idt_data def_idts[] = {
...
    ISTG(X86_TRAP_NMI,      asm_exc_nmi, IST_INDEX_NMI),
...
#ifdef CONFIG_X86_32
    TSKG(X86_TRAP_DF,       GDT_ENTRY_DOUBLEFAULT_TSS),
#else
    ISTG(X86_TRAP_DF,       asm_exc_double_fault, IST_INDEX_DF),
#endif
    ISTG(X86_TRAP_DB,       asm_exc_debug, IST_INDEX_DB),

#ifdef CONFIG_X86_MCE
    ISTG(X86_TRAP_MC,       asm_exc_machine_check, IST_INDEX_MCE),
#endif
...
#ifdef CONFIG_AMD_MEM_ENCRYPT
    ISTG(X86_TRAP_VC,       asm_exc_vmm_communication, IST_INDEX_VC),
#endif
...
}
```
* 在启动过程中初始化异常栈的处理
```cpp
start_kernel()
-> trap_init()
   -> cpu_init_exception_handling() /* Initialize TSS before setting up traps so ISTs work */
```
* 通过以下方式设定 PerCPU 的 `cpu_tss_rw` 的 `x86_tss.ist[]` 数组元素的值，指示的不同 IST 异常的栈顶（高地址）
```cpp
static inline void tss_setup_ist(struct tss_struct *tss)
{
    /* Set up the per-CPU TSS IST stacks */
    tss->x86_tss.ist[IST_INDEX_DF] = __this_cpu_ist_top_va(DF);
    tss->x86_tss.ist[IST_INDEX_NMI] = __this_cpu_ist_top_va(NMI);
    tss->x86_tss.ist[IST_INDEX_DB] = __this_cpu_ist_top_va(DB);
    tss->x86_tss.ist[IST_INDEX_MCE] = __this_cpu_ist_top_va(MCE);
    /* Only mapped when SEV-ES is active */
    tss->x86_tss.ist[IST_INDEX_VC] = __this_cpu_ist_top_va(VC);
}
...
/*
 * Setup everything needed to handle exceptions from the IDT, including the IST
 * exceptions which use paranoid_entry().
 */
void cpu_init_exception_handling(void)
{
    struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
    int cpu = raw_smp_processor_id();

    /* paranoid_entry() gets the CPU number from the GDT */
    setup_getcpu(cpu);

    /* IST vectors need TSS to be set up. */
    tss_setup_ist(tss);
    tss_setup_io_bitmap(tss);
    set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);

    load_TR_desc();

    /* Finally load the IDT */
    load_current_idt();
}
```
* 如果通过 PerCPU 的数组`exception_stacks[]`去查看每个 CPU 的 IST 栈，看到的是栈底（低地址）
* arch/x86/include/asm/cpu_entry_area.h
```cpp
#ifdef CONFIG_X86_64

#ifdef CONFIG_AMD_MEM_ENCRYPT
#define VC_EXCEPTION_STKSZ  EXCEPTION_STKSZ
#else
#define VC_EXCEPTION_STKSZ  0
#endif

/* Macro to enforce the same ordering and stack sizes */
#define ESTACKS_MEMBERS(guardsize, optional_stack_size)     \
    char    DF_stack_guard[guardsize];          \
    char    DF_stack[EXCEPTION_STKSZ];          \
    char    NMI_stack_guard[guardsize];         \
    char    NMI_stack[EXCEPTION_STKSZ];         \
    char    DB_stack_guard[guardsize];          \
    char    DB_stack[EXCEPTION_STKSZ];          \
    char    MCE_stack_guard[guardsize];         \
    char    MCE_stack[EXCEPTION_STKSZ];         \
    char    VC_stack_guard[guardsize];          \
    char    VC_stack[optional_stack_size];          \
    char    VC2_stack_guard[guardsize];         \
    char    VC2_stack[optional_stack_size];         \
    char    IST_top_guard[guardsize];           \

/* The exception stacks' physical storage. No guard pages required */
struct exception_stacks {
    ESTACKS_MEMBERS(0, VC_EXCEPTION_STKSZ)
};
```